\documentclass[10pt,conference]{IEEEtran}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{algorithm,algorithmicx}
\usepackage{algpseudocode}
\usepackage{blindtext, graphicx}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{float}
\usepackage[demo]{graphicx}
\usepackage{subfig}
%\usepackage{times}
\usepackage{cite}
%
\usepackage[absolute,showboxes]{textpos}
\usepackage{paralist}
\usepackage{rotating}
\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
\definecolor{Blue}{RGB}{0,29,193}
\definecolor{lightgray}{gray}{0.8}
\definecolor{darkgray}{gray}{0.6}
\definecolor{LightGray}{gray}{0.975}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}


\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}

\usepackage{multirow}
\usepackage{tcolorbox}% http://ctan.org/pkg/tcolorbox
\definecolor{mycolor}{rgb}{0.122, 0.435, 0.698}% Rule colour
\makeatletter
\newcommand{\mybox}[1]{%
  \setbox0=\hbox{#1}%
  \setlength{\@tempdima}{\dimexpr\linewidth}%
  \begin{tcolorbox}[colframe=mycolor,boxrule=0.5pt,arc=4pt,
      left=6pt,right=6pt,top=6pt,bottom=6pt,boxsep=0pt,width=\@tempdima]
    #1
  \end{tcolorbox}
}
\makeatother

\usepackage{times}
\usepackage{amsmath}
\usepackage[svgnames]{xcolor}
\usepackage[framed]{ntheorem}
\usepackage{framed}
\usepackage{tikz}
\usetikzlibrary{shadows}
%\newtheorem{Lesson}{Lesson}
\theoremclass{Lesson}
\theoremstyle{break}

% inner sep=10pt,
\tikzstyle{thmbox} = [rectangle, rounded corners, draw=black,
  fill=Gray,  drop shadow={fill=black, opacity=1}]
\newcommand\thmbox[1]{%
  \noindent\begin{tikzpicture}%
  \node [thmbox] (box){%
\begin{minipage}{.94\textwidth}%    
      \vspace{-1mm}#1\vspace{-1mm}%
    \end{minipage}%
  };%
  \end{tikzpicture}}

\let\theoremframecommand\thmbox
\newshadedtheorem{lesson}{Result}

%\hyphenation{op-tical net-works semi-conduc-tor}

\setlength{\textfloatsep}{0pt}

\begin{document}
\pagestyle{plain}

\title{\textbf{What's Wrong with
Topic Modeling?\\ (and How to Fix it Using Search-based SE)}}


\author{
\IEEEauthorblockN{Amritanshu Agrawal}
\IEEEauthorblockA{Computer Science, NCSU\\
Raleigh, North Carolina\\
aagrawa8@ncsu.edu}
\and
\IEEEauthorblockN{Tim Menzies}
\IEEEauthorblockA{Computer Science, NCSU\\
Raleigh, North Carolina\\
tim.menzies@gmail.com}
\and
\IEEEauthorblockN{Wei Fu}
\IEEEauthorblockA{Computer Science, NCSU\\
Raleigh, North Carolina\\
wfu@ncsu.edu}
}

% make the title area
\maketitle


\begin{abstract}
%\boldmath
  Topic Modeling is a method to find
  human-readable structures in large sets of unstructured SE data.
  A widely used topic modeler is Latent Dirichlet Allocation. We show that LDA on SE data suffers
  from ``order effects''; i.e. changing the order of the input data leads to
  large changes in the generated topics.

 Such order effects introduce a systematic error for all prior studies that use
 the contents of the generated topics to make conclusions about
 SE. Specifically: those conclusions are unstable since they would change if
 topic modeling is re-run on different orderings of the training data.

To solve this problem, this paper proposes LDADE: a combination of LDA and a
Search-Based Software Engineering method
(Differential Evolution) that automatically tunes LDA's
$<k,\alpha,\beta>$ parameters. LDADE is tested on  data from a programmer
information exchange site (Stackoverflow); title and abstract text of thousands
of SE papers; and software defect reports from NASA.  Results were collected
across different implementations of LDA (Python+Scikit-Learn, Scala+Spark); across
different platforms (Linux, Macintosh) and for different kinds of LDAs (the
traditional VEM method, or using Gibbs sampling) In all results, the pattern was
the same: (a)~before tuning, topic models were very unstable; and (b)~after
tuning the topics were far more stable.
  
The implications of this study for other software analytics tasks is now an open
and pressing issue. 
In how many domains can search-based SE dramatically improve software analytics?

\end{abstract}

%\begin{IEEEkeywords}
%Topic modeling, Stability, LDA, tuning, differential evolution.
%\end{IEEEkeywords}

%\IEEEpeerreviewmaketitle
\section{Introduction}
\label{sect: intro}
The current great challenge in software analytics is understanding unstructured data. As shown in Figure~\ref{fig: data}, most of the planet's 1600 Exabytes of data does not appear in structured sources (databases, etc)~\cite{nadkarni2014structured}. Rather
it is {\em unstrucutred} data, often in free text, and found in word processing
files, slide presentations, comments, etc etc. 

Such unstructured data does not have a pre-defined data model and is typically text-heavy. Finding insights among unstructured text is  difficult unless we can search, characterize, and classify the textual data in a meaningful way. One of the common techniques for finding related topics within unstructured text (an area called topic modeling) is Latent Dirichlet Allocation (LDA)~\cite{blei2003latent}.
Topic modeling is widely used in software engineering (see Table~\ref{tbl:survey2})
and many papers in recent years have reported  topic modeling results at numerous SE venues (see Figure~\ref{fig:survey1} and Table~\ref{tab:venues}).



%% Topic models built by LDA are non-deterministic in behaviour. Due to non-determinism, topics generated by LDA are not stable~\cite{oliveto2010equivalence, barua2014developers}. It learns the various distributions which is a problem of Bayesian inference~\cite{blei2003latent}. We are talking about model instability itself rather than results instability. Many industries, working on a product development in text analytics have been widely using it. They want better results in their product. Menzies et al.~\cite{menzieslocal} showed how model instability can give rise to large effort estimations across various projects. And we do need to avoid these large effort estimations.

%% LDA takes certain set of parameters which try to take us closer to cluster distributions. But it is an art to find the right parameters setting that control the choices within a data miner. But it is very impractical to get the right parameters settings due to larger search space of configurations. Nevertheless, we rarely tuned LDA since we reasoned that a LDA’s default tunings have been well-explored by the developers of those algorithms (in which case tuning would not lead to stable topic models). Due to larger search space of configurations, it is not even feasible to find such tunings. And that is why, LDA is usually used with off-the-shell default parameters. Many researchers have agreed to the idea of using different configurations in LDA. But, we have seen rare work done when it comes to tune the important parameters of LDA.


\begin{figure}[!b]
  \captionsetup{justification=centering}
  \includegraphics[width=3in]{./fig/data.png}
  \caption{Data Growth 2005-2015. From~\cite{nadkarni2014structured}.}
  \label{fig: data}
\end{figure}
%\rule{\textwidth}{0.4mm}


\noindent
Researchers can use topic models in one of two ways.
Firstly,
topics may be used as {\em feature selector} that finds useful inputs
which are then used by, say, a classifier to characterize different kinds of software (e.g. buggy or not~\cite{chen2016topic}).
In this mode, no human need ever read the generated topics and instabilities
in generated topics is less of an issue.

Secondly, researchers may present and reflect on the generated topics in order to
  offer some insight into the structure of the data.  In this second mode,
  researchers present and discuss the specifics of the generated topics in order
  to defend particular conclusions.

  This paper is about a 
  systematic error relating to this second mode.
  Specifically, we show that the generated topics from Latent Dirichlet
  Allocation (LDA), a widely-used topic modeling algorithm in SE, are subject to
  ``order effects''.  That is, if the order of the input examples is changed,
  the generated topics will also change-- often dramatically so.  Hence, any
  conclusion based on an order effect is unstable since that conclusion is the
  result of a (randomly selected) input ordering. 





\begin{figure}[!htbp]
  \centering
  \resizebox{0.4\textwidth}{!}{\includegraphics[width=\linewidth]{./fig/survey.png}}
  \caption{Topic modeling in SE: recent papers since 2007.}
  \label{fig:survey1}
\end{figure}




\renewcommand\arraystretch{1.2}
\begin{table*}[!t]
\tiny
  \centering
    \begin{tabular}{|c|c|c|c|c|c|c|p{6.5cm}|p{2.5cm}|}
        \hline 
        \begin{tabular}[c]{@{}c@{}}\textbf{Reference} \\\textbf{ID}\end{tabular} & \textbf{Year} & \textbf{Citations} & \textbf{Venues} & \begin{tabular}[c]{@{}c@{}}\textbf{Mentions} \\\textbf{instability} \\\textbf{in LDA?} \end{tabular} &\begin{tabular}[c]{@{}c@{}} \textbf{Talks about} \\\textbf{using-off-the} \\\textbf{shell parameters?}\end{tabular}&\begin{tabular}[c]{@{}c@{}} \textbf{Does} \\\textbf{tuning?}\end{tabular} & \multicolumn{1}{c|}{Conclusion}  & \multicolumn{1}{c|}{Tasks / Use cases}\\ \hline
        ~\cite{rao2011retrieval} & 2011 & 112 & WCRE & Y & Y & N & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation. \end{tabular} & \begin{tabular}[c]{@{}l@{}}Bug Localisation\end{tabular}\\ [0.5ex]\hline
        ~\cite{oliveto2010equivalence} & 2010 & 108 &MSR& Y & Y & N & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation. Reported their results using multiple\\ experiments.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Traceability Link recovery\end{tabular}\\ [0.5ex]\hline
        ~\cite{barua2014developers} &2014& 96 & ESE & Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation. Choosing right set of parameters is a\\ difficult task.\end{tabular}& \begin{tabular}[c]{@{}l@{}}StackOverflow Q\&A data analysis\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{panichella2013effectively} & 2013&75&ICSE & Y & Y & Y  & \begin{tabular}[c]{@{}l@{}}Uses GA to tune parameters. They determine the near-optimal configuration for LDA in\\ the context of only some important SE tasks.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Finding near-optimal configurations\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{galvis2013analysis} &2013& 61 &ICSE& Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Software Requirements Analysis\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{hindle2011automated} &2011& 45 & MSR & Y & Y & N  & \begin{tabular}[c]{@{}l@{}}They validated the topic labelling techniques using multiple experiments.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Software Artifacts Analysis\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{guzman2014users} & 2014 & 44 &RE& Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Requirements Engineering\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{thomas2011mining} &2011& 44 &ICSE & Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Open issue to choose optimal parameters.\end{tabular}& \begin{tabular}[c]{@{}l@{}}A review on LDA\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{chen2012explaining} &2012& 35 &MSR & Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Choosing the optimal number of topics is difficult.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Software Defects Prediction\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{thomas2014studying} & 2014 & 35 &SCP& Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Software Artifacts Analysis\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{thomas2014static} &2014& 31 &ESE& Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Choosing right set of parameters is a difficult task.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Software Testing\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{bajracharya2009mining} &2009 &29 & MSR& Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation and accepted to the fact their results\\ were better because of the corpus they used.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Software History Comprehenion\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{lohar2013improving} &2013& 27 &ESEC/FSE &Y & Y & Y  & \begin{tabular}[c]{@{}l@{}}Explored Configurations using LDA-GA.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Traceability Link recovery\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{linares2013exploratory} &2013& 20 &MSR& Y & Y & N  & \begin{tabular}[c]{@{}l@{}}In Future, they planned to use LDA-GA.\end{tabular}& \begin{tabular}[c]{@{}l@{}}StackOverflow Q\&A data analysis\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{koltcov2014latent} & 2014 & 15 & WebSci& Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Social Software Engineering\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{hindle2012relating} & 2012 & 13 &ICSM& Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation (Just with no. of topics).\end{tabular}& \begin{tabular}[c]{@{}l@{}}Software Requirements Analysis\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{grant2013using} & 2013 & 13 &SCP& Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Their work focused on optimizing LDA’s topic count parameter.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Source Code Comprehension\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{fu2015automated} &2015& 6 & IST & Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation. Choosing right set of parameters is a\\ difficult task.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Software re-factoring\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{garousi2016citations} &2016& 5 &\begin{tabular}[c]{@{}c@{}}CS Review\end{tabular}& Y & Y & N  & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Bibiliometric and citation analysis\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{le2014predicting} &2014& 5 & ISSRE& N & Y & N  & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Bug Localisation\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{nikolenko2015topic} & 2015 &3 &JIS& Y & Y & N  & \begin{tabular}[c]{@{}l@{}}They improvised LDA into ISLDA which gave stability across different runs.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Social Software Engineering\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{sun2015msr4sm} &2015& 2 &IST& Y & Y & Y  & \begin{tabular}[c]{@{}l@{}}Explored Configurations using LDA-GA.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Software Artifacts Analysis\end{tabular}\\ [0.5ex]
        \hline
        ~\cite{chen2016topic} &2016& 0 &JSS& N & Y & N  & \begin{tabular}[c]{@{}l@{}}Explored Configurations without any explanation. Choosing right set of parameters is a\\ difficult task.\end{tabular}& \begin{tabular}[c]{@{}l@{}}Software Defects Prediction\end{tabular}\\ [0.5ex]
        \hline
\end{tabular}
\caption{A sample of the recent literature on using topic modeling in SE. Note that some of these papers are widely-cited. }
\label{tbl:survey2}
\end{table*}


\begin{table}[!t]
\renewcommand\arraystretch{0.8}
\begin{center}
\scriptsize
\begin{tabular}{|c|l|c|}
\hline
\begin{tabular}[c]{@{}c@{}}\textbf{Venue}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textbf{Full Name}\end{tabular}     & \multicolumn{1}{l|}{\textbf{Count}} \\ \hline
ICSE & International Conference on Software Engineering  & 4  \\ \hline
\begin{tabular}[c]{@{}c@{}}CSMR-\\WCRE\\ / SANER \end{tabular}   & \begin{tabular}[c]{@{}l@{}}International Conference on Software Maintenance,\\ Reengineering, and Reverse Engineering / International \\Conference on Software Analysis,Evolution, and \\Reengineering\end{tabular} & 3  \\ \hline
\begin{tabular}[c]{@{}c@{}}ICSM\\ / ICSME\end{tabular}   & \begin{tabular}[c]{@{}l@{}}International Conference on Software Maintenance / \\International Conference on Software Maintenance and \\Evolution \end{tabular}  & 3\\ \hline
ICPC & International Conference on Program Comprehension  & 3  \\ \hline
ASE      & \begin{tabular}[c]{@{}l@{}}International Conference on Automated Software\\ Engineering  \end{tabular} & 3  \\ \hline
ISSRE      & \begin{tabular}[c]{@{}l@{}}International Symposium on Software Reliability\\ Engineering \end{tabular} & 2  \\ \hline
MSR      & \begin{tabular}[c]{@{}l@{}}International Working Conference on Mining \\Software Repositories \end{tabular} & 8  \\ \hline
OOPSLA  & \begin{tabular}[c]{@{}l@{}}International Conference on Object-Oriented \\Programming, Systems, Languages, and Applications \end{tabular}  & 1 \\ \hline
FSE/ESEC  &  \begin{tabular}[c]{@{}l@{}}International Symposium on the Foundations of Software \\Engineering / European Software Engineering Conference \end{tabular}  & 1                                    \\ \hline
TSE                                 & IEEE Transaction on Software Engineering   & 1                                     \\ \hline
IST                                 & Information and Software Technology                                 & 3                                       \\ \hline
SCP                                 & Science of Computer Programming                               & 2                                       \\ \hline
ESE                                 & Empirical Software Engineering                         & 4                                       \\ \hline
\end{tabular}
\end{center}
\caption{SE Venues that publish  on Topic Modeling.}
\label{tab:venues}
\end{table}
%\rule{\textwidth}{0.4mm}

%\begin{figure}[!b]
%  \centering


To fix this problem,
we proposes LDADE: a  combination of LDA and a search-based optimizer (differential evolution, or DE)~\cite{storn1997differential})
that automatically tunes LDA's \mbox{$<k,\alpha,\beta>$} parameters. This paper tests LDADE on:
\bi
\item Data from a programmer information exchange site (StackOverflow);
  \item Title and abstract text of
    9291 of SE papers;
  \item And software defect reports from NASA.
    \ei
    Using that data, we explore these research questions:  
  %%   Results were collected across different implementations of LDA (Python+ScikiLearn, Scala+Spark); across different platforms (Linux, Macintosh) and for different kinds of LDAs (the traditional VEM method, or using  Gibbs sampling)
  %% In all results, the pattern was the same:  (a)~before tuning, topic models were very unstable; and (b)~after tuning
  %% the generated topics were far more stable across different orderings of the inputs. 
\begin{compactitem}
\item \textbf{RQ1}: \textbf{Are the default settings of LDA misleading?} We will show that using the default settings of LDA for SE data can lead to systematic errors due to topic modeling instability since  stability scores start to drop after using $n=5$ terms per topic.
    \item \textbf{RQ2}: \textbf{Does tuning with DE improve the stability scores of LDA?} In our work, we will show dramatic improvement in the stability scores with the tuned parameters found automatically by DE. We would strongly recommend tuning for future LDA studies.
    \item \textbf{RQ3}: \textbf{Do different data sets
      need different configurations to make LDA stable?} We will show that DE finds different ``best''
      parameter settings for different data sets. This means that tuning needs to be repeated each time topic
      modeling is applied to  each new data set.
    \item \textbf{RQ4}: \textbf{Is tuning easy?} We have seen that one of the simplest multiobjective optimizers (differential evolution~\cite{storn1997differential}) works the same with the frontier size of 10 other than the recommended {\em10 times the number of tuned parameters (here, 3)}. In  terms  of  the  search  space  explored  via  tuning,  finding stable  parameters  for  topic  models  is  easier  that  standard optimization tasks.
    \item \textbf{RQ5}: \textbf{Is tuning impractically slow?}
      Tuning with DE makes LDA around five times slower- which is not an alarming
      speed reduction given modern cloud computing environments.
    \item \textbf{RQ6}: \textbf{Should topic modeling be used “off-the-shelf” with their default tunings?}
      Based on these findings, our answer to this question is an emphatic ``no''. We can see little reason to use “off-the-shelf ” LDA
\end{compactitem}
Before proceeding, we offer two clarifications.
Firstly, as
witnessed by the central columns of Table~\ref{tbl:survey2},
many prior papers have
commented that the results of a topic modeling analysis can be effected by
tuning the control parameters of LDA.  Yet a repeated pattern in the literature
is that, despite these stated concerns,
researchers rarely take the next step
to find ways to better control tunings.
To the best of our knowledge, apart from this paper, there are no reports in the SE literature
of simple reproducible automatic methods that can resolve the problem of unstable topics.

Secondly, while this specifics of this paper are about taming instability in topic modeling,
we also think that this work makes a more general point.
      This is the second time we have applied DE to automatically
      adjust the control parameters of an algorithm
      used in software analytics. Previously~\cite{fu2016tuning},
      we showed  that (a)~tuning software defect predictors can lead to large improvements of the performance
      of those predictors; and that (b)~those tunings are different in different data sets; so (c)~it is important to apply
      automatic tuning as part of the analysis of any new data set. Note that those old conclusions are the same
      as for this paper. These two results suggests that is time to revisit
      past results to check:
      \bi
    \item
       Just
      how poorly have we been using software analytics
      in the past?
    \item
      And how much better can we improve future results using methods taken from search-based
      software engineering?
      \ei


%% only two authors acknowledged the impact of
%% tunings. In fact  35\% papers in our 57 papers did not adjust the ``off-the-shelf''
%% tunings  of the LDA. Of  the others 65\%, 83\% papers, explored the
%% configurations space with brute force\footnote{u nean a grid search? or a ``engineering judgement''
%%   to set a few param?}. Their was not an effective and faster way
%% of choosing the right set of parameters. Those papers, which accepted the
%% problem of instability in LDA, came to some conclusions which can be found in
%% Table \ref{tb:survey2}. Conclusions of remaining papers are categorized in the
%% following subsections.



%YYY grader, harman, icse tunings.

%% More generally
%% Wei et al.~\cite{fu2016tuning} have shown that one of the simplest \textit{\textbf{multiobjective optimizers (differential evolution}}~\cite{storn1997differential}) works very well for tuning in certain kinds of software analytics. There has been success stories where DE has helped in tuning the parameters~\cite{bazi2014differential, chiha2012tuning}. Prior to this work, our intuition was that tuning would change the behavior of LDA, to some degree. Also, we suspected that tuning would take so long time and be so CPU intensive that the benefits gained would not be worth the effort. We have now seen large instability in 2 data miners, namely, defect predictors and LDA. Results have shown that we can contain the instability to a large extent using DE. So, this makes us wonder Search-Based Software Engineering (SBSE) techniques can help us.

%% The results of this paper show that the above points are false since:
%% \begin{compactitem}
%%   \item Tuning LDA is remarkably simple;
%%   \item And can dramatically improve the stability of LDA model.
%% \end{compactitem}

%% Based on answers to these questions, we can surely mitigate the instability of non-deterministic LDA. LDA should not be used with “off-the-shelf” default tunings. Any future paper on LDA should include a tuning study. Tuning needs to be repeated whenever data or goals are changed.

%% Topic Modeling have been used in various spheres of SE. Sun et al. reported a survey of LDA's usage to support various SE tasks between 2003 and 2015~\cite{sun2016exploring}. The number of papers published across top major conferences and journals are listed in Table \ref{tab:venues}. The survey in figure \ref{fig: survey} show that there is an increasing concern in this area.


%% \begin{compactitem}
%%     \item Topic modeling is applied in various SE tasks, including source code comprehension, feature location, software defects prediction, developer recommendation, traceability link recovery, re-factoring, software history comprehension, software testing and social software engineering.
%%     \item There are works in Requirements Engineering where it was necessary to analyze the text and come up with the important topics~\cite{asuncion2010software, thomas2014studying, massey2013automated}.
%%     \item People have used topic modeling in prioritizing test cases, \& identifying the importance of test cases~\cite{hemmati2015prioritizing, zhang2015inferring, yang2015predicting}.
%%     \item Increasingly, it has become very important to have automated tools to do a Systematic Literature Review (SLR) ~\cite{tsafnat2014systematic}. We found these papers~\cite{restificar2012inferring,alreview,marshall2013tools} who have used clustering algorithms (topic modeling) to do SLR.
%% \end{compactitem}

%% Since topic modeling has been widely used in various spheres of SE, and it is really important to generate stable topics. Prior Papers who reports LDA may have been severely compromised. This gives all of us the motivation of generating stable topics in software engineering which will make product development faster, cheaper and accurate.

%YYY amrtih: is instability widely acknowledge

\section{Related Work}

\subsection{About Tuning: Important and Ignored}
\label{sect: tuning}

The impact of tuning are well understood in the theoretical machine learning literature~\cite{bergstra2012random}.  When we tune a
data miner, what we are really doing is changing how a learner applies its
heuristics. This means tuned data miners use different heuristics, which means
they ignore different possible models, which means they return different models;
i.e. how we learn changes what we learn.

Yet issues relating to
tuning are poorly addressed in the software analytics literature.  Wei et al.~\cite{fu2016tuning} surveyed hundreds of recent SE papers in that area
of software defect prediction from static code attributes. They found that most SE
  authors do not take steps to explore tunings (rare exception:~\cite{tantithamthavorn2016icse}). For example, Elish et
  al.~\cite{elish2008predicting} compared support vector machines to other data
  miners for the purposes of defect prediction. That paper tested different
  “off-the-shelf” data miners on the same data set, without adjusting the
  parameters of each individual learner.  Yet Wei et al.
  showed that (a)~finding useful tunings for software defect prediction is remarkedly
  easy using differential evolution;  (b)~different data sets require
  different tunings; hence, for software defect prediction,  (c)~tuning is essential for  all
  new data sets.


Accordingly,  we are now engaged in an on-going research project that explored tuning for software analytics.
\bi
\item Find some data mining widely used in SE;
\item Check the conclusions of that data miner under different parameter tunings;
\item See if those different tunings significantly change the results of that learner;
\item Look for ways to better manage the exploration of those tunings.  \ei This
  paper explores these four steps in the context of topic modeling. 


  

\subsection{About Topic Modeling}\label{sect:tm}

Latent Dirichlet Allocation (LDA) is a generative statistical model that allows
sets of observations to be explained by unobserved groups that explain why some
parts of the data are similar. It learns the various distributions (the set of
topics, their associated word probabilities, the topic of each word, and the
particular topic mixture of each document).
What makes topic modeling interesting is that these algorithms scale to very
large text corpuses.  For example, in this paper, we apply to all of Wikipedia,
as well as two other large text corpuses in software engineering.


Figure~\ref{fig:lda} illustrates topic generation from StackOverflow.
From these words, LDA explores two probability distributions:
\bi
\item $\alpha=P(k|d)$; probability of topic $k$ in  document $d$.
\item $\beta=P(w|k)$; probability of word $w$ in topic $k$; 
\ei
  Initially, $\alpha,\beta$ may be set randomly as follows:
each word in a document was generated by first randomly picking a topic (from
the document’s distribution of topics) and then randomly picking a word (from
the topic’s distribution of words). Successive iterations of the algorithm 
count the implications of prior sampling which, in turn,  incrementally updates $\alpha,\beta$.

Apart from $\alpha,\beta$, the other parameters that define LDA
are:
\bi
%\item $u$ = number of update iterations;
\item $k$ = number of topics.
\ei
Researchers have suggested that the important parameters which affect LDA the most are those above mentioned. But there is $u$, which is number of update iterations. Since this one does not affect much, we have not used it to tune. 


\begin{figure}[!t]
%~\hrule~
  
  
%%      \scriptsize
%%      \noindent
%      Keywords from 15 documents:
%%      \be
%      \item Hadoop, Big Data, HBase, Java, Spark, Storm, 
%        Cassandra
%        \item
%          NoSQL, MongoDB, Cassandra, HBase, Postgres,
%          \item
% Python, scikit-learn, scipy, numpy, statsmodels, 
%   pandas,
% \item
%   R, Python, statistics, regression, probability,
%   \item
% machine learning, regression, decision trees, 
%   libsvm,
% \item
% Python, R, Java, C++, Haskell, 
%   programming languages,
% \item
%   statistics, probability, mathematics, theory,
%   \item
% machine learning, scikit-learn, Mahout, 
%   neural networks,
% \item
% neural networks, deep learning, Big Data, 
%   artificial intelligence,
%\item
% Hadoop, Java, MapReduce, Big Data,
% statistics, R, statsmodels,
% \item
% C++, deep learning, artificial intelligence, 
%   probability,
% \item
%   pandas, R, Python,
%   \item
%     databases, HBase, Postgres, MySQL, MongoDB,
%     \item
% libsvm, regression, support vector machines
%\ee


\scriptsize
\begin{center}
\begin{tabular}{c|c|c|c|c}
 
        \begin{tabular}[c]{@{}c@{}}Topic: String\\ Manipulation\end{tabular}    &\begin{tabular}[c]{@{}c@{}}Topic:\\ Function\end{tabular}    &\begin{tabular}[c]{@{}c@{}} Topic: OO \\Programming\end{tabular} &\begin{tabular}[c]{@{}c@{}}Topic: UI \\Development\end{tabular}&\begin{tabular}[c]{@{}c@{}}Topic: File \\Operation\end{tabular} \\\hline
string&function& class& control&file \\
charact&paramet& method& view&directori\\
encod&pass& object& event&path\\
format&return& call& button&folder\\
convert&argument& interfac& click&creat\\\hline
\end{tabular}
\end{center}
\caption{Example (from~\cite{oliveto2010equivalence}) %grus15
of generating topics from StackOverflow. For each topic, we show just the five most heavily
  weighted words.}\label{fig:lda}

\end{figure}

%\begin{center}
%\begin{tabular}{c|c|c|c}
 
%           Topic 0 &Topic 1 &Topic 2 &Topic 3\\\hline
%Java& R& HBase &regression\\
%Big Data& statistics& Postgres& libsvm\\
%Hadoop& Python& MongoDB& scikit-learn\\
%deep learning& probability& Cassandra& machine learning\\
%artificial intelligence& pandas& NoSQL& neural networks\\\hline
%\end{tabular}
%\end{center}
%\caption{Example (from~\cite{oliveto2010equivalence}) %grus15
%of generating topics (at bottom) from the
%  keywords from 15 document
%  (shown at top). For each topic, we show just the five most heavily
%  weighted words.}\label{fig:lda}
%\end{figure}


\subsection{About Order Effects}

\noindent
This paper uses tuning to fix ``order effects'' in topic modeling. Langley~\cite{GENNARI198911} defines such effects as follows:
\begin{quote}
{\em A learner $L$ exhibits an order effect on a training set of experiences $T$ if there exist
two or more orders of $T$ for which $L$ produces different knowledge structures.}
\end{quote}
Many learners exhibit order effects; e.g. certain incremental clustering algorithms generate different
clusters, depending on the order with which they explore the data~\cite{GENNARI198911}.
Hence, some algorithms survey the space of possible models across numerous
random divisions of the data (e.g. Random Forests~\cite{Breiman2001}).

From the description offered above in \S\ref{sect:tm},
we can see (a)~how topic modeling might be susceptible to order effects and (b)~how such order
effects might be tamed.
\bi
\item
  In the above description, $k$, $\alpha,\beta$ is initialized at random
then updated via an incremental re-sampling process. Such incremental updates are prone to order effects.
\item
  One technique to reduce the effect of different data orderings is to not initialize, $k$, $\alpha,\beta$ to some
  large value. The trick when applying this technique is that different data sets will requiring different
  initializations; i.e. the tuning process will have to be repeated for each new data set.
\ei
  
%%   Apart fro order effects, another reason for instability within LDA are the random choices
%%   made within Another reason for ins  
%% \subsection{About Topic Modeling and LDA}


%% Note that if LDA considers the training data $T$ in another ordering, other topics might be discovered.
%% Later in this article, when we explore  {\bf RQ1}, we will see that for SE data that this instability can be substantial.


%% %YYY need 5 lines on VEM and GS

%% Under the hood, LDA uses Bayesian inference to represent and update
%% $P_1,P_2$ which, in turn my use  alternative inference techniques like Gibbs
%% sampling~\cite{wei2006lda, griffiths2004finding} or variational expectation
%% maximization (VEM)~\cite{minka2002expectation}. These techniques in LDA try to
%% maximize the resulting lower bound on the log
%% likelihood~\cite{blei2003latent}. Note that the expected complete log likelihood of the
%% data have many local maximas, which leads to different distributions and in turn
%% leads to instability in the output.

%% %% The distributions which are found with the
%% %% help of LDA, resulting from the same dataset with the same vocabulary and model
%% %% parameters, any differences between them are entirely due to the randomness in
%% %% inference techniques~\cite{koltcov2014latent}. This randomness affects word and
%% %% document ratios. There is a problem of finding the optimal number of clusters,
%% %% but different configuration parameters may lead us to the stable topics.

%% %% The topics learned by LDA sometimes are difficult to interpret by end
%% %% users~\cite{yang2015improving, panichella2013effectively}. We want topics to be
%% %% finer grained (more number of topics) or coarse grained (less number of topics)
%% %% according to the use case. Rationality about the topics instability is important
%% %% if an industry is using topic modeling in all their use
%% %% cases~\cite{lau2014machine, o2015analysis}. We do not want vaguely defined
%% %% topics.

%% Recent work with online learning LDA~\cite{hoffman2010online} propose algorithms
%% that, empirically,
%% converge quickly to its final $P_1,P_2$ values.  %YYY do i have to that rightfaster than batch collapsed Gibbs
%% The external validity section of this paper will test if these fast convergence methods remove the instability problem.
%% %YY is this the gibbs stuff?

%We observed that this is just not happening with only particular programming language or particular library. This instability is happening irrespective of the tools in which LDA has been implemented. Some of the papers mentioned using GibbsLDA++ written in C++ and they observed the same instability~\cite{lukins2008source, tian2009using, guzman2014users}. Some papers mentioned about using Python implementation of LDA and observed the same instability~\cite{guzman2014users}. LDA implemented in JAVA language produced the same incoherent topics~\cite{martin2015app, hindle2011automated}. And, we observed this problem is in the Scikit-Learn version of LDA, implemented in Python, as well as Spark Mllib library.



\subsection{WorkArounds for LDA Instability Problem}
\label{sect: solutions}
%YYY
In order to understand how other researchers have explored LDA instability,
in April 2016, we searched scholar.google.com for the conjunction of “lda” and “topics” or “stable” or
“unstable” or “coherence”. Since 2012, there are  189 such papers, 57
of which are related to software engineering results. Of those papers:
%YYY makes no sense to me
\bi
\item 28
mention instability in LDA. %(more details can
%be found at \href{https://goo.gl/Bpc6Vb}{\textit{https://goo.gl/Bpc6Vb}}).
\item Of those 28, despite mentioning stability problems,
  10 papers used LDA's ``off-the-shelf'' parameters;
  \item The  other papers used some combination of manual adjustment or some
under-explained limited exploration of tunings based on ``engineering judgment''
(i.e. some settings guided by the insights of the researchers).
\item
Only 3 of the authors acknowledge that tuning might have a large impact
on the results.
\ei
Overall, there was little systematic exploration of tuning and LDA in the SE literature.
Instead, researchers relied on other methods that are less suited to automatic reproduction
of prior results.

%YYY dont understand this
In the literature, Researchers ~\cite{maskeri2008mining, martin2015app, guzman2014users}
    manually accessed the topics and then used for further experiments. Some
    made use of Amazon Mechanical Turk to create gold-standard coherence
    judgements~\cite{lau2014machine}. This solution is related to results
    stability rather than doing model stability
    Note that this workaround takes extensive manual effort and time.

    %YYY again. dont get it.
Another way is by incorporating user knowledge to the corpus. SC-LDA~\cite{yang2015improving} can handle different kinds of knowledge such as word correlation, document correlation, document label and so on. We know user knowledge is valuable but highly subjective and we seek fully automated methods.

%UYY need to sort this
Some researchers~\cite{panichella2013effectively, lohar2013improving, sun2015msr4sm}
used Genetic Algorithm (GA) to tune the parameters. And some others~\cite{galvis2013analysis, tian2009using}
achieved higher stability by just increasing the number of cluster size.
We seek LDA model stability and LDA-GA uses stochastics. But we found tuning
  combined with Wei et al.~\cite{fu2016tuning} work to have better advantage
  than the other solutions. And in our work, we have used this solution.
 
%\end{lesson}


%% \subsubsection{\textbf{Supervised}}
%% \label{sub:supervised}
%% \hfill

%% \noindent\fbox{
%%     \parbox[][][s]{\linewidth-0.35cm}{%
%%     \textbf{Solution 3}:
%%     We found so many improvised frameworks of LDA namely, \textit{Labeled LDA}~\cite{ramage2009labeled}, \textit{Dirichlet Forest LDA}~\cite{andrzejewski2009incorporating}, \textit{Logic LDA}~\cite{andrzejewski2011framework}, \textit{Quad-LDA}~\cite{newman2011improving}, \textit{NMF-LDA}~\cite{xie2015incorporating}, \textit{Interactive Topic Modeling} (ITM)~\cite{hu2014interactive}, \textit{MRTF}~\cite{daume2009markov} and many more, helping in generating better stable topics.
%%     }%
%% }

%% \mybox{We will not be able to use any of the above supervised solutions as our datasets are not labeled. And all these frameworks work best with the classified datasets. This is going to be our future work which we will be addressing next.}

\section{Methods}
\label{sect:evaluation}
This section describes our evaluation methods for measuring instability as well as the optimization
methods used to reduce that instability.


\begin{table}[!t]
\begin{center}
\begin{tabular}{c|r|r}
  \begin{tabular}[c]{@{}c@{}}\multicolumn{2}{c}{\textbf{~}}\end{tabular} & \multicolumn{2}{c}{\textbf{Size}} \\
    \cline{2-3}
         & \textbf{Before} & \textbf{After}\\ 
  \textbf{Data set}      & \textbf{Preprocessing} & \textbf{Preprocessing}\\ 
        \hline
        PitsA & 1.2 MB & 292 KB \\ 
        \hline
        PitsB & 704 KB & 188 KB \\
        \hline
        PitsC & 143 KB & 37 KB \\ 
        \hline
        PitsD & 107 KB & 26 KB \\ 
        \hline
        PitsE & 650 KB & 216 KB \\
        \hline
        PitsF & 549 KB & 217 KB \\ 
        \hline
        Ciemap & 8.6 MB & 3.7 MB \\ 
        \hline
        StackOverflow & 7 GB & 527 MB \\ 
\end{tabular}
\end{center}
\caption{Statistics on our datasets. PitsA, PitsB, etc refer to the issues
from six different NASA projects.}
\label{tbl:dataset}
\end{table}
\begin{figure*}[!t]
  \begin{center}
    \includegraphics[width=13cm]{./fig/jaccard.png}
    \end{center}
  \caption{Example of topics overlap off size $n=5$ across multiple runs.}
  \label{fig:jaccard}
\end{figure*}

\begin{figure}[!htbp]
  \includegraphics[width=\linewidth]{./fig/alln.png}
  \caption{$\Re_n$ scores of 
  Figure~\ref{fig:jaccard} for $1 \le n \le 9$}
  \label{fig:alln}
\end{figure}

\subsection{Data Sets}
To answer our research questions, and to enable reproducibility of our results,
we use the three open source datasets summarized in Table~\ref{tbl:dataset} and described
below.

\textbf{PITS} is a text mining data set generated from NASA software project
and issue tracking system (PITS) reports~\cite{menzies2008improving,
  menzies2008automated}. This text discusses
bugs and changes found in big reports and  review patches.
Such issues are used
to manage quality assurance, to support communication
between developers. Topic modeling in PITS can be used
to identify the top topics which can
identify each severity separately. The dataset can be downloaded from the
PROMISE
repository~\cite{promiserepo}. Note that that data comes from six different
NASA projects, which we label as PitsA, PitsB, etc.
    
 \textbf{StackOverflow} is the flagship site of the Stack Exchange Network which
 features questions and answers on a wide range of topics in computer
 programming.
Topic modeling on StackOverflow is useful for finding patterns in programmer knowledge.
 This data can be downloaded from various
 sources\footnote{https://archive.org/details/stackexchange}. 
    
  \textbf{Citemap} contains titles and abstracts of papers from a
 database of 11 senior software engineering conferences from 1993-2013. from 1993-2013. This data was
 obtained in the form of an SQL dump from the work of Vasilescu et
 al.~\cite{vasilescu2013historical}.  This dataset can be downloaded
 from our website\footnote{https://github.com/ai-se/citemap/blob/master/citemap.csv}.

  For this study, all  datasets were preprocessed using the usual text mining filters~\cite{Feldman2007}:
\bi
\item
  Stop words removal using NLTK toolkit \footnote{http://www.nltk.org/book/ch02.html}~\cite{bird2006nltk} : ignore very common short words such as  ``and'' or ``the'' 
\item
  Porter's stemming filter~\cite{Porter1980}: delete uniformative word endings; e.g. after stemming, all the following words would be rewritten
  to ``connect'': ``connection'', ``connections'',
``connective'',          
``connected'',
  ``connecting''.
\item
  Tf-idf feature selection: focus on the 5\% of words that occur frequently,
  but only in small numbers of documents. If a word occours $w$ times
  and is found in $d$ documents  and there
  are $W,D$ total number of words and documents, then Tf-idf is scored
  as follows:
  \[
  \mathit{tfidf}(w,d)=   \frac{w}{W} *\log{\frac{D}{d}}\]
  \ei

  Table~\ref{tbl:dataset} shows the sizes of our data before and after pre-processing.
  These datasets are of different sizes and so are processed using different tools:
  \bi
\item PITS and Citemap is small enough to process on a single (four core) deskop machine
  using Scikit-Learn~\cite{pedregosa2011scikit} and Python.
  \item StackOverflow is so large (7GB) that its  processing requires extra hardware support.
 This study used a Spark and Mllib on a cluster of 45 nodes to
 reduce the runtime.
 \ei
  




\subsection{Similarity Scoring}
%YY this apra makes no sense
%% To evaluate the topics coherence or the cluster stability using LDA, there has
%% been number of evaluation measures proposed. There is a direct approach, by
%% asking people about topics, and an indirect approach by evaluating
%% \textit{pointwise mutual information (PMI)}~\cite{lau2014machine, o2015analysis}
%% between the topic words. PMI being an automatic method, we are not sure of exact
%% details of it which made us to not use this measure. We could not use direct
%% approach, due to resource limitation to ask an expert for the type of datasets
%% we used.

For this work, we assess topic model stability via the {\em median number overlaps of size $n$ words}, which we denote  $\Re_n$.

For this measurement, we first determine the maximum number of topics we will study. For that purpose,
we will study the $\mathit{size of topic}=9$ but the standard in the LDA literature~\cite{panichella2013effectively, lukins2010bug} says that top 10 terms per topic are ideal. We did not select $\mathit{size of topic}=10$ just because our results showed that the $\Re_9$ is negligible.

Next, for $1 \le n \le 9$, we will calculate the median size of the overlap,
computed as follows:
\bi
\item Let one {\em run} of our rig shuffle the order of the training data, then builds topic models using the data;
  \item $m$ runs of our rig executes $m$ copies of one run, each time using a different random number beet,
\item We say topics are stable,
when there are $x$ occurrences of  $n$ terms appearing in all the topics seen in the $m$ runs.
\ei


For example, consider the topics shown in Figure~\ref{fig:jaccard}. These are generated via four {\em runs} of our system.
For $n=5$, we note that Topic~0 of run1 scores $\frac{2}{4}=0.5$ since it shares 5 words with topics in only two runs out of four runs.
Repeating that calculation for the other run1 topics shows that:
\bi
\item Topic~1 of run1 scores $\frac{3}{4}=0.75$;
\item Topic~2 or run1 scores $\frac{1}{4}=0.25$;
\item Topic~3 of run1 scores $\frac{4}{4}=1$.
  \ei
  From this information, we can calculate
  $\Re_4$  (the
  {\em median number overlaps of size $n=5$ words}) as:
  \[
   \mathit{median}(0.5, 0.75, 0.25, 1) =0.625\]
  %YYY need an example of Fig5 here using the data of Figure4.
  Figure~\ref{fig:alln}
  shows the $\Re_n$ scores of 
  Figure~\ref{fig:jaccard} for $1 \le n \le 9$. 
  
 For the following analysis,
 %YYY do our graphs really use this terminology?
we distinguish between the \textbf{\textit{Raw  scores}} and the \textbf{\textit{Delta  score}}:
 \bi
\item The two \textbf{\textit{Raw  scores}} are the $\Re_n$ median similarity scores seen {\em before} and {\em after} tuning LDA;
\item The \textbf{\textit{Delta score}} is the difference between the two
  \textbf{\textit{Raw scores}}.  \ei 
  The pseudocode for these calculations
  is shown in Algorithm 1 with the default set of parameters). In the following
  description, superscript numbers denote lines in the pseudocode. The data ordering is
  shuffled every time LDA is run$^{5}$. Data is in the form of term frequency
  scores of each word per document. Shuffling is done in order to induce maximum
  variance among the ordering of data with  different runs of LDA. Topics$^{6}$ are a list of list which
  contains topics from all the different runs. A stability score is evaluated on
  every 10 runs (Fixed), and this process is continued 10 (Fixed) times. At the end, the median
  score is selected as the untuned final score$^{3-11}$.

%YYY this is weird. you use terminology different below. Finmal score. unify with the above terminology
% and werre is ``n''
% wrape words in \mathit{word}
\makeatletter
\algrenewcommand\ALG@beginalgorithmic{\footnotesize}
\algrenewcommand\algorithmiccomment[2][\normalsize]{{#1\hfill\(\triangleright\) #2}}
\makeatother
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}[!t]
    %\scriptsize
    %\label{algo: untuned}
    
    \begin{algorithmic}[1]
    %\label{algo: untuned}
    \Require $Data$, $\mathit{size of topic}$, $k$, $\alpha$, $\beta$ (Defaults) 
    \Ensure $Final\_Score$    
    \Function{ldascore}{ $\mathit{size of topic}$, $Data$}
        \State $Score \leftarrow \emptyset$
        \For{$j = 0$ to 10}
            \For{$i = 0$ to 10}
                \State $data \leftarrow$ \textbf{shuffle}($Data$)
                \State $Topics$.append(lda($k$,$\alpha$,$\beta$ ))
            \EndFor
            \State $Score$.append($Overlap$(Topics, $\mathit{size of topic}$, $l[0]$))
        \EndFor
        \State $Final\_Score \leftarrow $ median(Score)
        \State \textbf{return} $Final\_Score$
    \EndFunction
    %\medskip
    \caption{Pseudocode for untuned LDA with Default Parameters}
    \end{algorithmic}
\end{algorithm}


%YYY: who first proposed LDA... need a referece first time we mention if
\subsection{Tuning Topic Modeling with LDADE}
\label{sect: tuning}
LDADE is a combination of topic modeling (with LDA) and an optimizer (differential evolution, or DE) that adjusts
the parameters of LDA in order to optimize (i.e. maximize) similarity scores.

We choose to use DE after a literature search on search-based SE methods.
That literature mentions many optimizers: simulated
annealing~\cite{feather2002converging, menzies2007business}; various genetic
algorithms~\cite{goldberg1979complexity} augmented by techniques such as
differential evolution~\cite{storn1997differential}, tabu search and scatter
search~\cite{glover1986general, beausoleil2006moss, molina2007sspmo,
  nebro2008abyss}; particle swarm optimization~\cite{pan2008particle}; numerous
decomposition approaches that use heuristics to decompose the total space into
small problems, then apply a response surface methods~\cite{krall2015gale,
  zuluaga2013active}. Of these, the simplest ones are simulated annealing (SA)
and differential evolution (DE) and  our reading of the current literature is
that there are more advocates for differential evolution than SA. 

\begin{table}[!b]
    \begin{center}
{\scriptsize
\begin{tabular}{|l|l|l|p{3.5cm}|}
        \hline 
        \textbf{Parameters} & \textbf{Defaults} & \textbf{Tuning Range} & \textbf{Description}\\
        \hline
        $k$ & 10 & [10,100] & Number of topics or cluster size \\ 
        \hline
       $\alpha$ & None & [0,1] & Prior of document topic distribution. This is called alpha \\ 
        \hline
        $\beta$ & None & [0,1] & Prior of topic word distribution. This is called  beta \\

        \hline
        inferences  & VEM & VEM    & Sampling methods \\
                  &     & Gibbs Sampling &\\
        \hline
\end{tabular}
}
\end{center}
\caption{List of parameters tuned by this paper}
\label{tb:tuned}
\end{table}

\newpage
LDADE  adjusts the parameters of
Table~\ref{tb:tuned}. Most of the parameters were explained above. As to the rest:
\bi
\item VEM is the deterministic {\em variational EM} method that computes $\alpha,\beta$ via
  expectation maximization~\cite{minka2002expectation}.
\item Gibbs sampling~\cite{wei2006lda, griffiths2004finding} which is a Markov Chain Monte Carlo algorithm, is an approximate stochastic process for computing and updating $\alpha,\beta$.
  Topic modeling researchers in SE have argued that Gibbs leads to stabler models~\cite{layman16a,layman2016topic} (a claim which we test, below).
  \ei

We manually adjust these inference techniques according to different implementations across different platforms.
%YYY Does the layman reference does not appear in table1?
%YYY Why does LDAscore appear in algorithm2 and algorithm1


  
%YYY why do you only iteratre 3 times? surely you iterate till no improvement?
%YYY why is LDAScore here?
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}[!t]


 
  
    \begin{algorithmic}[1]
    \Require np$=10$, f$=0.7$, cr$=0.3$, iter$=3$, Goal $\in$ Finding maximum score
    \Ensure $S_{best}, final\_generation$
    \Function{DE}{$np,f,cr,iter, Goal$}
        \State  $Cur\_Gen \leftarrow \emptyset$
        \State $Population \leftarrow $InitializePopulation(np)
        \For{$i = 0$ to $np-1$}
            \State $Cur\_Gen$.append($Population$[i],ldascore($Population$[i], $\mathit{size of topic}$, $Data$)
        \EndFor
        \For{$i = 0$ to $iter$}
            \State $NewGeneration \leftarrow \emptyset$
            \For{$j = 0$ to $np-1$}
                \State $S_i \leftarrow $Extrapolate($Population$[j],Population,cr,f,np)
                \If{ldascore($S_i$) $\geq$ $Cur\_Gen$[j][1]}
                    \State $NewGeneration$.append($S_i$,ldascore($S_i$,$\mathit{size of topic}$, $Data$))
                \Else
                    \State $NewGeneration$.append($Cur\_Gen$[j])
                \EndIf
            \State  $Cur\_Gen \leftarrow NewGeneration$
            \EndFor
        \EndFor
        \State $S_{best} \leftarrow$ GetBestSolution($Cur\_Gen$)
        \State  $final\_generation \leftarrow Cur\_Gen$
        \State \textbf{return} $S_{best}, final\_generation$
    \EndFunction

    %YYY line numbers have changed
    %% \Function{ldascore}{$l$, $term$, $Data$}
    %%     \State $Topics \leftarrow \emptyset$
    %%     \For{$i = 0$ to 10}
    %%         \State $data \leftarrow$ \textbf{shuffle}($Data$)
    %%         \State $Topics$.append(lda($k=l[0]$,$\alpha=l[1]$,$\beta=l[2]$)) 
    %%     \EndFor
    %%     \State \textbf{return} $Overlap$(Topics, $term$,$l[0]$ )
    %% \EndFunction
    \Function{Extrapolate}{$old, pop, cr, f,np$}
        \State $a,b,c \leftarrow threeOthers$(pop, old)
        \State $newf \leftarrow \emptyset$
        \For{$i = 0$ to $np-1$}
            \If{$cr \leq$ random()}
                \State $newf$.append($old[i]$)
            \Else
                \If{typeof($old$[i])$ ==$ bool then}
                    \State $newf$.append(not $old[i]$)
                \Else 
                    \State $newf$.append(trim(i,($a$[i]+$f\ast$($b$[i] $-$ $c$[i]))))
                \EndIf
            \EndIf
        \EndFor
        \State \textbf{return} $newf$ 
    \EndFunction
    \caption{Pseudocode for DE with a constant number of evaluations}
    \end{algorithmic}
\end{algorithm}



Algorithm 2 shows LDADE's version of DE.  DE evolves a \textit{NewGeneration} of
candidates from a current Population.   Each candidate solution in the Population is a pair of
(Tunings, Scores). Tunings are selected from Table \ref{tb:tuned} and Scores
come similarly from Alogrithm 1$^{3-11}$. In Algorithm 2, LDA is only run as 1 rig$^{11 \& 12}$.



The main loop of DE$^{9}$ runs over the Population, replacing old items with new Candidates (if new candidate is better).
DE generates new Candidates via 
extrapolating$^{23}$ between current solutions in the frontier.. Three solutions a, b, c are
selected at random. For each tuning parameter i, at some probability cr, we
replace the old tuning $x_i$ with $y_i$. For booleans, we use $y_i = x_i$ (see
line 31). For numerics, $y_i = a_i + f \times (b_i - c_i)$ where f is a
parameter controlling crossover. The trim function$^{33}$ limits the new value
to the legal range min..max of that parameter.

The loop invariant of DE is that, after the zero-th iteration$^7$, the Population
contains examples that are better than at least one other candidate.
As the looping progresses, the Population is full of increasingly more valuable solutions
which, in turn, also improves the candidates, which are Extrapolated from the Population.
Hence, Vesterstrom and Thomsen~\cite{vesterstrom2004comparative} found DE to be
competitive with particle swarm optimization and other GAs.

Note that DEs have been
applied before for parameter tuning (e.g. see~\cite{omran2005differential,
  chiha2012tuning, fu2016tuning} ) but this is the first time they have been
applied to tune the configurations of LDA.

\section{Experimentation}


In this section,
 any result
from the smaller data sets (Pits and Citemap) come
from a Python implementation based on Scikit-Learn running on a single 4-core machine.
Also,
  any results from the larger data (StackOverflow) comes from a Scala implementation
  based on Mllib running on a 45 node Spark system (8 cores per node).
  
  Note that, for the most part, we defer the StackOverflow results to the {\em Threats to Validity}
  section where we will show that (a)~topic modeling instability
  exists across multiple platforms and  implementation languages; (b)~tuning
  can improve stabilities for different platforms and languages.

  



  %% YY legends hard to read . make none-bold
  %% why do they stop at X=9
  %% x-axis has to have the term \Re_n as well as the text ``number of topics overlaps``
\begin{figure}[!t]
  \begin{center}
  \includegraphics[width=\linewidth]{./fig/Vem_untuned.png}

  {\bf Figure~\ref{fig:delta}a:}  {\em Before} tuning: uses LDA's default parameters.

  \includegraphics[width=\linewidth]{./fig/raw_graph.png}

  {\bf Figure~\ref{fig:delta}b:}  {\em After} tuning: uses parameters learned by DE.

  

  \includegraphics[width=\linewidth]{./fig/tuned_delta_vem.png}

  {\bf Figure~\ref{fig:delta}c:}  {\em Delta = After - Before}. 
  
\end{center}
  \caption{{\bf RQ1, RQ2} stability results over ten repeated runs. In these figures, {\em larger} numbers
    are {\em better}.}\label{fig:delta}
\end{figure}
\subsection{\textbf{RQ1: Are the default settings of LDA misleading?}}


This first research question checks the core premise of this article-- that changes
in the order of training data dramatically effects the topics learned via LDA.
Note that if this is {\em not true}, then there would be no value-added to this paper.


Figure~\ref{fig:delta}a   plots $n$ vs $\Re_n$ for untuned  LDA.
Note that the  stability collapses after $n=5$ words. This means
  that any report of LDA topics that uses more than five words per topic will
  be changed, just by changing the order of the inputs. This is a significant result
  since the standard advice in the LDA papers~\cite{panichella2013effectively, lukins2010bug}
  is to report the top 10 words per topic. As shown in Figure~\ref{fig:delta}a, it would
  be rare that any such 10 word topic would be found across multiple rules.
  
 
\begin{lesson}
  Using the default settings of LDA for SE data can lead to systematic errors due to topic
  modeling instability. 
\end{lesson}




\subsection{\textbf{RQ2: Does tuning improve the stability scores of LDA?}}

 Figure~\ref{fig:delta}b and Figure~\ref{fig:delta}c shows the stability improvement
 generated by tuning.
   Tuning never
  has a negative effect (reduces stability) band often has a large positive effect--
  particular  after 5 terms overlap.
   The largest improvement  we
   was  in PitsD dataset which for up to 8 terms overlap was 100\% (i.e. was always
   found in all runs).
   Overall, after reporting topics of up to 7 words, in the majority case (66\%),
  those topics can be found in models generated using different input orderings.
  Accordingly, our answer to {\bf RQ2} is:

\begin{lesson}
Based on Figure \ref{fig:delta}, we strongly recommend tuning for future LDA studies.
\end{lesson}


\begin{figure*}[!htbp]
    \centering
    \begin{minipage}{.33\textwidth}
        \captionsetup{justification=centering,singlelinecheck=off}
        \includegraphics[width=\linewidth]{./fig/Parameters_variation_k.png}
        \caption{Datasets vs Parameter (k) variation.  Note that IQR=0 for  PitsB dataset where tuning
          always converged on the same final value. }
        \label{RQ3:k}
    \end{minipage}%
    \begin{minipage}{.33\textwidth}
        \captionsetup{labelsep=space,justification=centering,singlelinecheck=off}
        \includegraphics[width=\linewidth]{./fig/Parameters_variation_a.png}
        \caption{Datasets vs Parameter ($\alpha$) variation}
        \label{RQ3:a}
    \end{minipage}
    \begin{minipage}{.33\textwidth}
        \captionsetup{labelsep=space,justification=centering,singlelinecheck=off}
        \includegraphics[width=\linewidth]{./fig/Parameters_variation_b.png}
        \caption{Datasets vs Parameter ($\beta$) variation}
        \label{RQ3:b}
    \end{minipage}
\end{figure*}



\subsection{\textbf{RQ3: Do different data sets
      need different configurations to make LDA stable?}}

Figures \ref{RQ3:k}, \ref{RQ3:a}, and \ref{RQ3:b} show the results of tuning the $k,\alpha,\beta$
values within LDA. On display in each set of vertical bars are:
\bi
\item The median values generated across 10 tunings;
\item The inter-quartile range (IQR) of those tunings (the IQR is the 75th-25th percentile values and is a non-parametric measure of variation
  around the median value).
  \ei
  From these figures, it
clearly shows how tuning selects the different ranges of values of
parameters.
Some of the above numbers are far from the standard values recommended in the literature.
For example, Garousi et al. ~\cite{garousi2016citations} recommend using $k=67$ topics
yet in our data sets, best results were seen using $10 \ke k \le 24$.
Clearly:

\begin{lesson}
  Do not  reuse tunings suggested by other researchers from other data sets.
  Instead, use automatic tuning methods to find the best tuning parameters for the current data set.
\end{lesson}



\begin{figure*}[!t]
    \centering
  \begin{minipage}{.49\textwidth}
        \captionsetup{labelsep=space,justification=centering,singlelinecheck=off}
        \includegraphics[width=\linewidth]{./fig/Run_VEM_sci.png}
  \caption{VEM: Datasets vs Runtimes}
  \label{RQ5 VEM}
  \end{minipage}
  \begin{minipage}{.49\textwidth}
        \captionsetup{justification=centering,singlelinecheck=off}
        \includegraphics[width=\linewidth]{./fig/Run_gibbs_sci.png}
  \caption{Gibbs: Datasets vs Runtimes}
  \label{RQ5 Gibbs}
    \end{minipage}%
    
\end{figure*}

\subsection{\textbf{RQ4: Is  tuning  easy?}}


The DE literature
recommends using a population size $np$ that is ten times larger than the number of parameters being
optimized~\cite{storn1997differential}.  For example, when tuning $k,\alpha,\beta$,
the DE literature is recommending $np=30$.
Figure~\ref{fig:RQ4} explores $np=30$ vs the $np=10$ we use in Algorithm 2
(as well as some other variants of DE's F and CR parameters).
That figure shows results jsut for Citemap and, for space reasons, results
relating to other data sets are shown at https://goo.gl/HQNASF.
After reviewing the results from all the datasets, we can say that there isn't much of an improvement by using different F, CR, and Population size. So our all other experiments used $F=0.7$, $CR=0.3$ and $np = 10$.
Also:

\begin{lesson}
  In terms of the search space explored via tuning, finding stable parameters for
  topic models is easier that standard optimization tasks.
\end{lesson}

\begin{figure}[!htbp]
  \includegraphics[width=\linewidth]{./fig/citemap.png}
  \caption{Terms vs Delta Improvement using Different settings of DE}
  \label{fig:RQ4}
\end{figure}

\subsection{\textbf{RQ5: Is tuning impractically slow?}}

Search-based SE methods can be very slow. Harman et al. once needed 15 years of CPU time to find and verify
the tunings required for software clone detectors~\cite{wang2013searching}. Sayyad et al. routinely used $10^6$ evaluations (or more)
  of their models in order to extract products from highly constrained product lines~\cite{sayyad2013scalable}. Hence, before recommending
  any search-based method, it is wise to consider the runtime cost of that recommendation.

%YYY why so fast? io bound?
  Figures \ref{RQ5 VEM} and \ref{RQ5 Gibbs} and  shows in blue,red the runtimes required to run LDA untuned,tuned (respectively).
  The longer runtimes (in red) include the times required for DE to find the tunings. Overall, tuning slows down LDA by a factor of up to seven,
  but often less than five. Note that this is much less than we might expect from Algorithm 2 which contains multiple loops across Populations of different parameters.
  On investigation, we found that much of this inference is I/O bound (reading data and/or swapping memory from RAM to virtual disk).

  Hence, while tuning certain slows down LDA, the overall slowdown is not dramatic. Given the added benefit of tuning (stable topics),
  we conclude that:

  \begin{lesson}
    Tuning LDA is not impractically slow.
\end{lesson}

\subsection{\textbf{RQ6: Should data miners be used “off-the-shelf” with their  default  tunings?}}


  Figure~\ref{fig:delta} shows that there is much benefit in tuning.
  Figures \ref{RQ3:k}, \ref{RQ3:a}, and \ref{RQ3:b} show that
  the range of ``best'' tunings is very dataset-specific. Hence, for a new dataset,
  the off-the-shelf tunings
  may often fall far from the useful range.
  Figures \ref{RQ5 VEM} and \ref{RQ5 Gibbs} show that tuning is definitely
  slower than otherwise, but the overall cost is not prohibative.
  Hence:
  \begin{lesson}
    Overall, we can see little reason  to use ``off-the-shelf'' LDA.
    \end{lesson}
  %YYY check that the text for the lessons matches the text on p2
  
  %YYY why not a,b,k for fig6,7,8
 
 

\section{Threats to Validity}
\label{sect: validity}
The usual software analytics threats to validity hold for this paper:
\bi
\item
  Our conclusions assume that the goal of topic modeling is to produce stable topics
  that humans will browse and reflect on. As mentioned in the introduction, there are
  class of papers that do {\em not} do that. Rather, they use LDA as part of some internal
  process where the topics are intermediaries
  used to generate some other goal~\cite{chen2016topic}. As we said at the start
    of this paper, our results do not effect that kind of paper.
\item
The conclusions of this paper are based on a finite number of data sets and it is possible
that other data might invalidate our conclusions. As with all analytics papers,
all any researcher can do is make their conclusions and materials public, then encourage
other researchers to repeat/ refute/ improve their conclusions. For example, the footnotes of this contain all the urls where other researchers can download
our materials and explore the conclusions of this paper.
\ei

As to more specific threats to validity, one issue might be that our conclusions
on ``LDA'' are really quirks of a specific implementation.
To check this, it is insightful to compare our results with:
\bi
\item The Pits and Citemap results, executed in Scikit-Learn and Python running on
  a desktop machine.
\item The StackOverflow data set executed in Scala using Mllib running on a Spark cluster.
  \ei
  Another useful comparison is to change the internal of the LDA:
  \bi
\item Sometimes use VEM sampling;
\item Sometimes use Gibbs sampling;
  \ei
  Figure~\ref{gibbs_vem} compares the  VEM vs Gibbs sampling.
  and Figure
   \ref{python_spark} shows tuning results for StackOverflow, Citemap, and PitsA
   using Scala/Spark cluster (for results on other data sets, see
   https://goo.gl/UVaql1 and https://goo.gl/faYAcg)
   When compared with the Python/desktop results of
   Figure~\ref{fig:delta} we see the same patterns:
   \bi
 \item Tuning never makes stability worse.
 \item Sometimes, it dramatically improves it (in particular, see the Citemap results
   of  Figure~\ref{python_spark}).
   \ei
   That said, there are some deltas between VEM and Gibbs where it seems tuning
   is more important for VEM than Gibbs (evidence: the improvements seen after
   tuning are largest for the  VEM results of  Figure~\ref{gibbs_vem} and at  https://goo.gl/faYAcg).

   
\begin{figure}[!htbp]
  \captionsetup{justification=centering}
  \includegraphics[width=\linewidth]{./fig/gibbs_vem1.png}
  \caption{GIBBS vs VEM}
  \label{gibbs_vem}
\end{figure}
\begin{figure}[!htbp]
  \captionsetup{justification=centering}
  \includegraphics[width=\linewidth]{./fig/spark.png}
  \caption{Spark Results}
  \label{python_spark}
\end{figure}

Another threat to validity of this work is that it is a quirk of the control
parameters used within our DE optimizer.
We have some evidence that this is not the case.
Figure~\ref{fig:RQ4} and https://goo.gl/HQNASF explored a range of DE tunings and found
little difference across that range. Also, Table~V explores another choice within DE-- how
many evaluations to execute before terminating DEs. All the results in this paper use an
evaluation budget of 300 evaluations. Table~V
compares results across different numbers of evaluations. While clearly,
the more evaluations the better, there is little improvement after the
300 evaluations used in this paper.

%YYY dont beleive that. the time results are too good.

\begin{table}[!htbp]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
\textbf{Datasets\textbackslash Evaluations} & \textbf{100} & \textbf{200} & \textbf{300} &
\textbf{500} \\[0.5ex]
\hline
PitsA & 0.9 & 0.9 & 1.0 & 1.0\\ 
\hline
PitsB & 0.9 & 0.9 & 0.9 & 1.0 \\
\hline
PitsC & 0.9 & 1.0 & 1.0 & 1.0\\ 
\hline
PitsD & 0.9 & 1.0 & 1.0 & 1.0\\ 
\hline
PitsE & 0.9 & 0.9 & 1.0 & 1.0\\
\hline
PitsF & 0.9 & 0.9 & 0.9 & 0.9\\
\hline
Citemap & 0.67 & 0.67 & 0.77 & 0.77\\
\hline
StackOverflow & 0.6 & 0.7 & 0.8 & 0.8\\
\hline
\end{tabular}
\end{center}
\caption{Evaluations vs Stability Scores}
\label{tb:tablename1}
\end{table}

%% \textit{\textbf{Other measures for stability}}
%% The above work used the stability measure defined in \S\ref{sect:evaluation}.
%% Perh
%% One measure from the LDA literature~\cite{koltcov2014latent} that measures stability is
%% \textit{perplexity}. This measure is defined to be the inverse of the geometric mean per-word
%% likelihood. Perplexity shows how
%% well topic-word and word-document distributions predict new test samples. The
%% smaller the perplexity, the better (less uniform) is the LDA model. The usual
%% trend is that as the value of perplexity drops, the number of topics should
%% grow~\cite{koltcov2014latent}. Researchers caution that the value of perplexity
%% doesn't remain constant with different topic size and with dictionary
%% sizes~\cite{koltcov2014latent, zhao2015heuristic}. A lot depend on the code
%% implementation of perplexity and the type of datasets used. Since, we are using
%% different implementations of LDA as well as different datasets, we are not using
%% Perplexity as evaluation measure.

\section{Conclusion and Future Work}

Our exploration of the the research questions listed in the introduction shows that when doing topic modeling, analytics without parameter tuning are considered harmful and misleading. As more data getting generated day by day, it is really necessary to do accurate topic modeling. Now, with the help of tuning, we can generate stable topics to quite a good extent. Now we can delve into prior papers to improve their work. We can use the more stable LDA to find features and fed into a classifer to get improved precision and recall~\cite{chen2016topic,restificar2012inferring}

This paper showed that tuning improved the stability scores of LDA, sometimes the improvement is quite dramatic (about 80\%). This paper also highlighted that we can now select the right set of parameters for different datasets to get stable topics. This paper combined with Wei et al.~\cite{fu2016tuning} suggests that data miners should not be used off-the-shell with their default tunings.

As to future work, it is now important to explore the implications of these stable topics generated by LDA in a product development. We can now actually work with other workarounds mentioned in the section \ref{sub:supervised} with classified datasets. To improve stability, we can even try to improve the actual clusters to get better results.

This paper just investigated on stability of LDA using one optimizer. Hence, we can make no claim that DE is the best optimizer for all data miners. Rather, our point is that there exists at least some learners whose performance can be dramatically improved by at least one simple optimization scheme. And there are already claims of other unstable data miners like Decision Tree Learning, Neural Networks, and Bayesian Learning~\cite{zhang2005machine} where optimizers can mitigate instability into all these miners. We also hope that this work inspires much future work as this community develops and debugs best practices for tuning software analytics.


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\bibliographystyle{abbrv}
\medskip
\bibliography{sigproc}


% that's all folks
\end{document}


